{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model, layers and activation function for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "                             self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "                            self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                   np.sum(layer.weights *\n",
    "                                          layer.weights)\n",
    "\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                   np.sum(layer.biases *\n",
    "                                          layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/james/Datasets_Drive/semg_for_basic_hand_movement/Database_1/female_1.mat\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "(540000, 3)\n",
      "[0.0002986961254352518, 0.0002797453486948402, 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "dataDir = \"/media/james/Datasets_Drive/semg_for_basic_hand_movement/Database_1/\"\n",
    "dataFiles=['female_1.mat','female_2.mat','female_3.mat','male_1.mat','male_2.mat']\n",
    "dataFiles=['female_1.mat']\n",
    "\n",
    "data = [] #processed and normalised with pose_idx\n",
    "e = 2.718281828459045\n",
    "\n",
    "def normalize_arr(arr,i):\n",
    "    signal = arr.copy()\n",
    "    signal -= np.amax(arr)\n",
    "    signal = e**signal\n",
    "    signal /= np.sum(signal)\n",
    "    signal = np.clip(signal,1e-7,1e+7)\n",
    "    signal = signal.tolist()\n",
    "    signal.append(int(i/2))\n",
    "    return signal\n",
    "\n",
    "def get_channel_pair(chn1,chn2,i):\n",
    "    paired_data = []\n",
    "    label = list((np.array(chn1).T)[-1].T.astype(int))\n",
    "\n",
    "    chn1 =  list(np.delete(chn1,0,1))\n",
    "    chn2 =  list(np.delete(chn2,0,1))\n",
    "    # print(np.array(chn1).shape)\n",
    "    print(label)\n",
    "    # print('label[i]: ',label[i])\n",
    "    for chunk1 in chn1:\n",
    "        chunk2 = chn2[i].T\n",
    "        chunk1 = chunk1.T\n",
    "\n",
    "        while(len(chunk1)>0):\n",
    "            row=[chunk1[0],chunk2[0],label[5]]\n",
    "            paired_data.append(row)\n",
    "\n",
    "            chunk1 = list(chunk1)\n",
    "            chunk2 = list(chunk2)\n",
    "            chunk1.pop(0)\n",
    "            chunk2.pop(0)\n",
    "\n",
    "    return paired_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file in dataFiles:\n",
    "    data_path = dataDir+file\n",
    "    print(data_path)\n",
    "    mat = scipy.io.loadmat(data_path)\n",
    "    mat.pop(\"__header__\")\n",
    "    mat.pop(\"__version__\")\n",
    "    mat.pop(\"__globals__\")\n",
    "    i=0\n",
    "    for channel in mat: \n",
    "        if(i%2==0):\n",
    "            channel2 =  channel[:-1]\n",
    "            channel2 += '2'\n",
    "            sigs1 = mat[channel] \n",
    "            sigs2 = mat[channel2]\n",
    "            sigs1_norm = []\n",
    "            sigs2_norm = []\n",
    "\n",
    "            for signal in sigs1:\n",
    "                signal = normalize_arr(signal,i)\n",
    "                sigs1_norm.append(signal)\n",
    "            for signal in sigs2:\n",
    "                signal = normalize_arr(signal,i)\n",
    "                sigs2_norm.append(signal)\n",
    "\n",
    "            if i==0:\n",
    "                data = get_channel_pair(sigs1_norm,sigs2_norm,i)\n",
    "                # print(data)\n",
    "            else:\n",
    "                None\n",
    "                data_lc = get_channel_pair(sigs1_norm,sigs2_norm,i)\n",
    "                data += data_lc\n",
    "                # data = data_lc\n",
    "        i+=1\n",
    "print(np.array(data).shape)\n",
    "print(data[-3])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        channel1  channel2  label\n",
      "0       0.000271  0.000368      0\n",
      "1       0.000316  0.000308      0\n",
      "2       0.000286  0.000308      0\n",
      "3       0.000233  0.000324      0\n",
      "4       0.000227  0.000308      0\n",
      "...          ...       ...    ...\n",
      "539995  0.000284  0.000310      5\n",
      "539996  0.000299  0.000334      5\n",
      "539997  0.000299  0.000280      5\n",
      "539998  0.000348  0.000352      5\n",
      "539999  5.000000  5.000000      5\n",
      "\n",
      "[540000 rows x 3 columns]\n",
      "train labels:  [0 1 2 3 4 5]\n",
      "test labels:  [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = data.copy()\n",
    "df = pd.DataFrame(dataset, columns = ['channel1','channel2','label'])\n",
    "\n",
    "print(df)\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "dataset_train = df[mask]\n",
    "dataset_test = df[~mask]\n",
    "\n",
    "X = dataset_train[[\"channel1\",\"channel2\"]].to_numpy()\n",
    "y = dataset_train[\"label\"].to_numpy()\n",
    "\n",
    "X_test = dataset_test[[\"channel1\",\"channel2\"]].to_numpy()\n",
    "y_test =  dataset_test[\"label\"].to_numpy()\n",
    "print('train labels: ',dataset_train['label'].unique())\n",
    "print('test labels: ',dataset_test['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "431580\n",
      "5\n",
      "5\n",
      "108440\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def reshape_sample_X(n,df_data):\n",
    "    # get n channel 1 and n channel 2 into 1 sample\n",
    "    chn1 = df_data['channel1'].to_numpy()\n",
    "    chn2 = df_data['channel2'].to_numpy()\n",
    "    label = df_data['label'].to_numpy()\n",
    "    i = 0\n",
    "    newShape_data = []\n",
    "    newShape_label = []\n",
    "    time = 0\n",
    "    while i < chn1.size:\n",
    "        #currently  the size is 540000, tack batch 20 signals, the loop will rn 27000 times\n",
    "        reshape_data = np.concatenate((chn1[i:i+n], chn2[i:i+n]))\n",
    "        if(len(reshape_data)==n*2):\n",
    "            newShape_data.append(reshape_data)\n",
    "            newShape_label.append(label[i])\n",
    "        i += n\n",
    "        if(i>=chn1.size):\n",
    "            # print(reshape_data)  \n",
    "            print(label[i-20])\n",
    "        time += 1\n",
    "    print(i)\n",
    "    newShape_data = np.array(newShape_data)\n",
    "\n",
    "    return newShape_data\n",
    "def reshape_sample_y(n,df_data):\n",
    "    # get n channel 1 and n channel 2 into 1 sample\n",
    "    chn1 = df_data['channel1'].to_numpy()\n",
    "    chn2 = df_data['channel2'].to_numpy()\n",
    "    label = df_data['label'].to_numpy()\n",
    "    i = 0\n",
    "    newShape_data = []\n",
    "    newShape_label = []\n",
    "    time = 0\n",
    "    while i < chn1.size:\n",
    "        #currently  the size is 540000, tack batch 20 signals, the loop will rn 27000 times\n",
    "        reshape_data = np.concatenate((chn1[i:i+n], chn2[i:i+n]))\n",
    "        if(len(reshape_data)==n*2):\n",
    "            newShape_data.append(reshape_data)\n",
    "            newShape_label.append(label[i])\n",
    "        i += n\n",
    "        if(i>=chn1.size):\n",
    "            # print(reshape_data)  \n",
    "            print(label[i-20])\n",
    "        time += 1\n",
    "    newShape_label = np.array(newShape_label)\n",
    "    return newShape_label\n",
    "\n",
    "# print(df)\n",
    "df = pd.DataFrame(dataset, columns = ['channel1','channel2','label'])\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "dataset_train = df[mask]\n",
    "dataset_test = df[~mask]\n",
    "\n",
    "n=20\n",
    "X =  reshape_sample_X(n,dataset_train)\n",
    "y =  reshape_sample_y(n,dataset_train)\n",
    "\n",
    "X_test =reshape_sample_X(n,dataset_test)\n",
    "y_test =  reshape_sample_y(n,dataset_test)\n",
    "\n",
    "# X = batchTrain[0]\n",
    "# y = batchTrain[1]\n",
    "\n",
    "# X = reshape_sample(20,df)\n",
    "\n",
    "\n",
    "# X = dataset_train[[\"channel1\",\"channel2\"]].to_numpy()\n",
    "# y = dataset_train[\"label\"].to_numpy()\n",
    "\n",
    "# X_test = dataset_test[[\"channel1\",\"channel2\"]].to_numpy()\n",
    "# y_test =  dataset_test[\"label\"].to_numpy()\n",
    "# print('train labels: ',dataset_train['label'].unique())\n",
    "# print('test labels: ',dataset_test['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00027135, 0.00031622, 0.00028555, ..., 0.0003322 , 0.00031568,\n",
       "        0.00029998],\n",
       "       [0.00018988, 0.00022127, 0.00023887, ..., 0.00035862, 0.00031568,\n",
       "        0.0003322 ],\n",
       "       [0.00029293, 0.00023887, 0.00018044, ..., 0.00031568, 0.00029242,\n",
       "        0.0003322 ],\n",
       "       ...,\n",
       "       [0.00029118, 0.00037576, 0.00034808, ..., 0.0003178 , 0.00033443,\n",
       "        0.00039981],\n",
       "       [0.00034808, 0.00041611, 0.00044919, ..., 0.00033443, 0.00028697,\n",
       "        0.00033443],\n",
       "       [0.00024986, 0.00023744, 0.00029118, ..., 0.00030979, 0.00037992,\n",
       "        0.00038974]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def reshape_sample(n,df_data):\n",
    "#     # get n channel 1 and n channel 2 into 1 sample\n",
    "#     chn1 = df_data['channel1'].to_numpy()\n",
    "#     chn2 = df_data['channel2'].to_numpy()\n",
    "#     label = df_data['label'].to_numpy()\n",
    "#     i = 0\n",
    "#     newShape_data = []\n",
    "#     newShape_label = []\n",
    "#     time = 0\n",
    "#     while i < chn1.size:\n",
    "#         #currently  the size is 540000, tack batch 20 signals, the loop will rn 27000 times\n",
    "#         reshape_data = np.concatenate((chn1[i:i+n], chn2[i:i+n]))\n",
    "#         if(len(reshape_data)==n*2):\n",
    "#             newShape_data.append(reshape_data)\n",
    "#             newShape_label.append(label[i])\n",
    "#         i += n\n",
    "#         if(i>=chn1.size):\n",
    "#             # print(reshape_data)  \n",
    "#             print(label[i-20])\n",
    "#         time += 1\n",
    "#     print(i)\n",
    "#     print(newShape_label)\n",
    "#     print(len(newShape_label))\n",
    "#     print(newShape_data)\n",
    "#     print(len(newShape_data))\n",
    "#     print(time)\n",
    "#     newShape_data = np.array(newShape_data)\n",
    "#     newShape_label = np.array(newShape_label)\n",
    "#     res_batch = np.array([newShape_data,newShape_label])\n",
    "#     res_batch = newShape_data\n",
    "#     res_batch = newShape_label\n",
    "\n",
    "\n",
    "#     print(newShape_data.size)\n",
    "#     print(len(newShape_data))\n",
    "#     # return newShape_data\n",
    "#     return res_batch\n",
    "\n",
    "# # print(df)\n",
    "# df = pd.DataFrame(dataset, columns = ['channel1','channel2','label'])\n",
    "# mask = np.random.rand(len(df)) < 0.8\n",
    "# dataset_train = df[mask]\n",
    "# dataset_test = df[~mask]\n",
    "\n",
    "# n=20\n",
    "# batchTrain =  reshape_sample(n,dataset_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "iterate\n"
     ]
    }
   ],
   "source": [
    "og_X = np.array(X.copy())\n",
    "print(type(og_X))\n",
    "while(og_X.size!=0):\n",
    "    print('iterate')\n",
    "    og_X =np.array( [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_draw = []\n",
    "loss_draw = []\n",
    "data_loss_draw= []\n",
    "reg_loss_draw = []\n",
    "learn_rate_draw = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.183, loss: 1.792 (data_loss: 1.792, reg_loss: 0.000), lr: 0.02\n",
      "epoch: 100, acc: 0.167, loss: 1.792 (data_loss: 1.792, reg_loss: 0.000), lr: 0.019999010049002574\n",
      "epoch: 200, acc: 0.167, loss: 1.792 (data_loss: 1.792, reg_loss: 0.000), lr: 0.019998010197985302\n",
      "epoch: 300, acc: 0.167, loss: 1.792 (data_loss: 1.792, reg_loss: 0.000), lr: 0.019997010446938183\n",
      "epoch: 400, acc: 0.167, loss: 1.792 (data_loss: 1.792, reg_loss: 0.000), lr: 0.01999601079584623\n",
      "epoch: 500, acc: 0.167, loss: 1.792 (data_loss: 1.792, reg_loss: 0.000), lr: 0.01999501124469445\n",
      "epoch: 600, acc: 0.167, loss: 1.792 (data_loss: 1.792, reg_loss: 0.000), lr: 0.01999401179346786\n",
      "epoch: 700, acc: 0.167, loss: 1.792 (data_loss: 1.792, reg_loss: 0.000), lr: 0.01999301244215147\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/james/Documents/AI/chatbot/NN_basic copy.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m dense0\u001b[39m.\u001b[39mforward(activation3\u001b[39m.\u001b[39moutput)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Perform a forward pass through the activation/loss function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# takes the output of second dense layer here and returns loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m data_loss \u001b[39m=\u001b[39m loss_activation\u001b[39m.\u001b[39;49mforward(dense0\u001b[39m.\u001b[39;49moutput, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Calculate regularization penalty\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m regularization_loss \u001b[39m=\u001b[39m \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     loss_activation\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mregularization_loss(dense1) \u001b[39m+\u001b[39m \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     loss_activation\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mregularization_loss(dense2)\n",
      "\u001b[1;32m/home/james/Documents/AI/chatbot/NN_basic copy.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=455'>456</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation\u001b[39m.\u001b[39moutput\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=456'>457</a>\u001b[0m \u001b[39m# Calculate and return loss value\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=457'>458</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss\u001b[39m.\u001b[39;49mcalculate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput, y_true)\n",
      "\u001b[1;32m/home/james/Documents/AI/chatbot/NN_basic copy.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=378'>379</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate\u001b[39m(\u001b[39mself\u001b[39m, output, y):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=379'>380</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=380'>381</a>\u001b[0m     \u001b[39m# Calculate sample losses\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=381'>382</a>\u001b[0m     sample_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(output, y)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=383'>384</a>\u001b[0m     \u001b[39m# Calculate mean loss\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=384'>385</a>\u001b[0m     data_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(sample_losses)\n",
      "\u001b[1;32m/home/james/Documents/AI/chatbot/NN_basic copy.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=413'>414</a>\u001b[0m     correct_confidences \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=414'>415</a>\u001b[0m         y_pred_clipped \u001b[39m*\u001b[39m y_true,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=415'>416</a>\u001b[0m         axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=416'>417</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=418'>419</a>\u001b[0m \u001b[39m# Losses\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=419'>420</a>\u001b[0m negative_log_likelihoods \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39;49mlog(correct_confidences)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/james/Documents/AI/chatbot/NN_basic%20copy.ipynb#X11sZmlsZQ%3D%3D?line=420'>421</a>\u001b[0m \u001b[39mreturn\u001b[39;00m negative_log_likelihoods\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create Dense layer with 2 input features and 64 output values\n",
    "# n=2\n",
    "dense1 = Layer_Dense(n*2, 64, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 30, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "\n",
    "dense3 = Layer_Dense(30, 15, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation3 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense0 = Layer_Dense(15, 6)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    dense2.forward(dense1.output)\n",
    "    dense3.forward(dense2.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    activation3.forward(dense3.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense0.forward(activation3.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    data_loss = loss_activation.forward(dense0.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_activation.loss.regularization_loss(dense1) + \\\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "        acc_draw.append(accuracy)\n",
    "        loss_draw.append(loss)\n",
    "        data_loss_draw.append(data_loss)\n",
    "        reg_loss_draw.append(regularization_loss)\n",
    "        learn_rate_draw.append(optimizer.current_learning_rate)\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense0.backward(loss_activation.dinputs)\n",
    "    \n",
    "    activation3.backward(dense0.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.update_params(dense0)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc_draw.pop(0)\n",
    "loss_draw.pop(0)\n",
    "data_loss_draw.pop(0)\n",
    "reg_loss_draw.pop(0)\n",
    "learn_rate_draw.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "step = 100\n",
    "plt.plot(range(0,epoch,step),acc_draw, label='acc')\n",
    "plt.plot(range(0,epoch,step),loss_draw, label='loss')\n",
    "plt.plot(range(0,epoch,step),data_loss_draw, label='data_loss')\n",
    "plt.plot(range(0,epoch,step),reg_loss_draw, label='reg_loss')\n",
    "plt.plot(range(0,epoch,step),learn_rate_draw, label='learn rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.166, loss: 1.792\n"
     ]
    }
   ],
   "source": [
    "# Validate the model\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "dense2.forward(dense1.output)\n",
    "dense3.forward(dense2.output)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "activation2.forward(dense2.output)\n",
    "activation3.forward(dense3.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense0.forward(activation3.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense0.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "\n",
    "\n",
    "# epoch: 10000, acc: 0.947, loss: 0.217 (data_loss: 0.157, reg_loss: 0.060), lr: 0.019900507413187767\n",
    "# validation, acc: 0.830, loss: 0.435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bluetooth\n",
    "\n",
    "nearby_devices = bluetooth.discover_devices(lookup_names=True)\n",
    "print(\"Found {} devices.\".format(len(nearby_devices)))\n",
    "\n",
    "for addr, name in nearby_devices:\n",
    "    print(\"  {} - {}\".format(addr, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
